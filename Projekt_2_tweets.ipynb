{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt 2 Tweet sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem projektu jest stworzenie modelu do oceny czy tweet jest pozytywny, negatywny lub neutralny. W tym celu dokonano przetworzenia otrzymanych tweetów do postaci listy wyrazów, która została ograniczona przez zdefiniowaną listę słów nie oddających emocji. Klasyfkikacji dokonano za pomocą klasyfikatora Naiwnego Bayesa. Dla zbioru testowego otrzymano trafność ponad 56%\n",
    "\n",
    "Pierwszym krokiem jest import potrzebnych bibliotek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RE_EMOTICONS_POS = re.compile(\"(:-?\\))|(:p)|(:d+)|(;-?\\))|(<3)\")\n",
    "RE_EMOTICONS_NEG= re.compile(\"(:-?\\()|(:/)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejnym krokiem będzie przygotowanie funkcji \"czyszczącej\" tweety z niepotrzebnych danych czy znaków. Takimi przykładami mogą być wyrazy zaczynające się @ (usunięto ten znak), które odnoszą sie do konkretnego konta Twitter, linki do stron, dodatkowe spacje oraz usunięcie hashtagu (#) sprzed wyrazów. Dodatkowo na samym początku duże litery są zamieniane na małe. Emotikony zgrupowane zostały w pozytywne i negatywne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def firstClean(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','WWW_URL',text)\n",
    "    \n",
    "    text = re.sub(RE_EMOTICONS_POS, 'EMO_POS',text)\n",
    "    text = re.sub(RE_EMOTICONS_NEG, 'EMO_NEG',text)\n",
    "    \n",
    "    text = re.sub(r'@([^\\s]+)', r'\\1',text)\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    text = text.strip('\\'\"?,.')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "W celu usunięcia słów, które nie mają nacechowania emocjonalnego lub są częstow używane w j. angielskim użyto listy słów ze strony http://www.ranks.nl/stopwords. Z pobranych słów wcześniej usunięto słowa przeczące (not, arent itp), które mogą mieć znaczenia dla negatywnych tweetów. Dodano również do tej listy znacznik dotyczący adresów URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createStopWordsList():\n",
    "    stopWords = []\n",
    "    stopWords.append('WWW_URL')\n",
    "    \n",
    "    fp = open('stopWordList.txt', 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        w = line.strip()\n",
    "        stopWords.append(w)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    \n",
    "    return stopWords\n",
    "\n",
    "stopWords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomocnicza funkcja usuwająca ciąg takich samych liter, gdy jest ich więcej niż dwa np looove lub loooove zostanie zastąpnione na loove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deleteTwoPlus(text):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja rozkładająca tweet na osobne wyrazy. Do tworzącej przez tę funkcję listy nie zostają dodane wyrazy zaczynające się od cyfry oraz wyrazy znajdujące się na liście wyrazów odrzuconych (stopWords). Przed porównaniem usuwane są ciągi powtarzających się liter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizerTweet(text):\n",
    "    featureVector = []\n",
    "    tokens = text.split()\n",
    "    for t in tokens:\n",
    "        t = deleteTwoPlus(t)\n",
    "        val = re.search(r\"^[a-z][a-z0-9]*$\", t)\n",
    "        if(t in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(t)\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja zbierająca dwie funkcje przetwarzająca tweety. Czyszcząca tweet oraz tokenizująca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTweet(text):\n",
    "    text = firstClean(text)\n",
    "    return tokenizerTweet(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozpoczęcie przetwrzania zbioru treningowego. Wywyoływana jest funkcja tworząca listę słów odrzucanych. Każdy tweet ze zbioru treningowego jest przetwarzany oraz tworzona jest lista tokenów na podstawie tokenów z każdego tweetu. Tokeny oraz kategoria tweetu (negative, positive, neutral) dodawane są do listy tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopWords = createStopWordsList() \n",
    "\n",
    "tweets_load = pd.read_table(\"train.csv\",sep = ',')\n",
    "tweets = []\n",
    "featureList = []\n",
    "\n",
    "for index, row in tweets_load.iterrows():\n",
    "    category = row['Category']\n",
    "    tweet = row['Tweet']\n",
    "    vec = processTweet(tweet)\n",
    "    featureList.extend(vec)\n",
    "    tweets.append((vec,category))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dla każdego tweetu tworzona jest lista zawierająca wartości true lub false  w zależności czy występuje w nim wyraz z listy featureList. \n",
    "\n",
    "Przykład:\n",
    "{\n",
    "    'contains(amendment)': True          \n",
    "    'contains(bush)': True          \n",
    "    'contains(scotus)': True \n",
    "    .....\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFeatures(text):\n",
    "    features = {}\n",
    "    tweet_words = set(text)\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z featureList usuwane są duplikaty słów. Następnie dla każdego tweetu treningowego tworzone są vecotry opisane przy funkcji extractFeatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureList = list(set(featureList))\n",
    "trainingSet = nltk.classify.util.apply_features(extractFeatures, tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczymy klasyfikator Naiwnego Bayesa na podstawie zbioru treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NBClassifier = nltk.NaiveBayesClassifier.train(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatnim krokiem jest stworzenie pliku wyjściowego dla zbioru testowego, w którym staramy się przewidzieć jakie nacechowanie emocjonalne mają tweety. Opisane kroki doprowadziły do uzyskania ponad 56% trafności dla zbioru testowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetsTest = pd.read_table(\"test.csv\",sep = ',',dtype={'Id': object})\n",
    "\n",
    "ofile  = open('result.csv', 'w')\n",
    "writer = csv.writer(ofile, delimiter=',')\n",
    "\n",
    "writer.writerow(('Id','Category'))\n",
    "for index, row in tweetsTest.iterrows():\n",
    "    if (index < 4000):\n",
    "        id_n = row['Id']\n",
    "        tweet = row['Category']\n",
    "        result = NBClassifier.classify(extractFeatures(processTweet(tweet)))\n",
    "        writer.writerow((id_n,result))\n",
    "\n",
    "ofile.close()    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
